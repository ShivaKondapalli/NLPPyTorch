{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation with LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaKondapalli/NLPPyTorch/blob/master/Text_Generation_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1ZCoVD_0nog",
        "colab_type": "text"
      },
      "source": [
        "In this Notebook, we shall generate text from **Francis Ford Coppola's Masterpiece Apocalypse Now**. We will use \n",
        "an **LSTM network** with 2 layers to build  a **Languague model**.  We'll surely have bots writing scripts for us in the \n",
        "future!\n",
        "\n",
        "First, let's get a high level walkthrough of what the model: **LSTM** , and an exciting techinque in NLP called LANGAUGE MODELLING are all about.  We will then see how how the two are used to generate text. \n",
        "\n",
        "The code reference for this, almost verbatim the same is : \"[link text](https://github.com/LeanManager/NLP-PyTorch/blob/master/Character-Level%20LSTM%20with%20PyTorch.ipynb)\" \n",
        "\n",
        "\n",
        "**LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzAlda2I69d4",
        "colab_type": "text"
      },
      "source": [
        "Install Pypdf2 to extract data from pdf file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D38jGWA06zIl",
        "colab_type": "code",
        "outputId": "42c6b184-d4da-4e6e-e556-14fcca55b7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "pip install PyPDF2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3z8Di5ew0Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import all libraries\n",
        "\n",
        "import torch\n",
        "from PyPDF2 import PdfFileReader\n",
        "import re\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Ncixiaxfhi",
        "colab_type": "code",
        "outputId": "98e1e9d6-c4c0-4716-de63-f5a2eb6603ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('The power of the GPU is with you!')\n",
        "else:\n",
        "  print('Trainingi your model forever!')\n",
        "  \n",
        "  \n",
        "torch.cuda.get_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The power of the GPU is with you!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qILy8VYyNxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"Apocalypse_Now.pdf\"\n",
        "\n",
        "file_object = open(path, 'rb')\n",
        "\n",
        "def read_data(file_object):\n",
        "\n",
        "    data = PdfFileReader(file_object)\n",
        "\n",
        "    if data.isEncrypted:\n",
        "        data.decrypt('')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_all_data(data):\n",
        "\n",
        "    text = ''\n",
        "\n",
        "    for i in range(1, data.numPages):\n",
        "\n",
        "        pageobj = data.getPage(i)\n",
        "\n",
        "        text += pageobj.extractText()\n",
        "\n",
        "    return text\n",
        "  \n",
        "\n",
        "# Extracting file object\n",
        "file_object = read_data(\"Apocalypse_Now.pdf\")\n",
        "\n",
        "# reading data into data_list, every page of our pdf is a string\n",
        "text = extract_all_data(file_object)  # length = 138, numofPages."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuGutukJ6gG7",
        "colab_type": "code",
        "outputId": "0bd50a77-88b2-4e5c-8e5f-774e91ebc0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"function to convert text to lowercase, retain alphabets, replace hyphenation with space\"\"\"\n",
        "\n",
        "    text = text.replace('-', '')\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate({ord(i): None for i in '!\"?();´'})\n",
        "    text = text.replace('--', '')\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "Clean_Text = clean_text(text)\n",
        "\n",
        "chars = tuple(set(Clean_Text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "char2int = {w: i for i, w in int2char.items()}\n",
        "\n",
        "# each word in tokens converted to an integer.\n",
        "encoded = np.array([char2int[char] for char in Clean_Text])\n",
        "print(encoded)\n",
        "print(encoded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19 19 37 ... 36  3  0]\n",
            "(158684,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA-e3HjWA-nG",
        "colab_type": "code",
        "outputId": "ffaa8959-1603-43ce-8a32-7dd68732f96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot\n",
        " \n",
        "ch = np.array([[char2int['w']]])\n",
        "print(ch)\n",
        "print(ch.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[44]]\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEKpdXRhBW94",
        "colab_type": "code",
        "outputId": "d51413d8-38b5-478e-df3d-59200cdfd3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def get_batches(arr, n_seqs, n_steps):\n",
        "\n",
        "    batch_size = n_seqs * n_steps\n",
        "\n",
        "    # total number of characters in our batch, in vision, number of images in a batch\n",
        "    # is batch size, 64, 32 and so on. total # of batches is the length of training set/batch_size.\n",
        "\n",
        "    n_batches = len(arr)//batch_size  # total num of batches to make from encoded array.\n",
        "\n",
        "    # following is total characters to keep from encoded array to have full batches which are evenly spaced.\n",
        "\n",
        "    # number of batches * characters per batch will give the number of characters to slice form our encoded array.\n",
        "    arr = arr[:n_batches * batch_size]\n",
        "    # print(arr)\n",
        "    # print(arr.shape)\n",
        "\n",
        "    # we will be splitting this array enc into N number of batches each having seq_len number of characters.\n",
        "\n",
        "    arr = arr.reshape((n_seqs, -1))\n",
        "    # print(enc)\n",
        "    # print(enc.shape)\n",
        "\n",
        "    # sq_len * number of batches, this should be equal to the second dimension of our array.\n",
        "    # print(40 * 396) # product of this with n_seqs_per_batch gives total number of characters.\n",
        "\n",
        "    # 10 * 40 window 10 * 15480\n",
        "\n",
        "    # we now need to split our features\n",
        "\n",
        "    for n in range(0, arr.shape[1], n_steps):\n",
        "\n",
        "        x = arr[:, n:n+n_steps]\n",
        "\n",
        "        y = np.zeros_like(x)\n",
        "\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + n_steps]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "\n",
        "        yield x, y\n",
        "        \n",
        "batches = get_batches(encoded, 10, 40)\n",
        "x, y = next(batches)\n",
        "# five rows,\n",
        "print(x[:5])\n",
        "print('')\n",
        "print(y[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[19 19 37 35 38 31 36 13  8 26 19  5 20  8 31 37 19 19 36  8 35 26 40 19\n",
            "   0  8 20  3 38 43 19 32 46 19 52 23  9 18 19 23]\n",
            " [11 51 51 19 46  1 34 55 39 46 19 52 23  9 18 19  9  1 34 43 32 55 23 42\n",
            "  19 16 32 51 32 43 11  9 18 29  6 24 53 23 55 19]\n",
            " [47 43 23  9 46  6 47 34 51 51 19 34 47 19 11 55 39 19  1 14 14 19 43  1\n",
            "  19 43 53 23 19 46 32 39 23 29 19 24 44  1 19 11]\n",
            " [46 45  1 47 23 29 19 21 32 51  4  1  9 23 19 43 11 50 23 46 19 32 43 19\n",
            "  11 55 39  6  1 47 23 55 46 19 43 53 23 19 17  1]\n",
            " [38 26 26  8 35  0 31  1 46 43 51 18 19 14  9  1 16 19 43 53 23 19 32 16\n",
            "  47  9 32 55 43 46 19 44 53 23 55 19 43 53 23 18]]\n",
            "\n",
            "[[19 37 35 38 31 36 13  8 26 19  5 20  8 31 37 19 19 36  8 35 26 40 19  0\n",
            "   8 20  3 38 43 19 32 46 19 52 23  9 18 19 23 11]\n",
            " [51 51 19 46  1 34 55 39 46 19 52 23  9 18 19  9  1 34 43 32 55 23 42 19\n",
            "  16 32 51 32 43 11  9 18 29  6 24 53 23 55 19 11]\n",
            " [43 23  9 46  6 47 34 51 51 19 34 47 19 11 55 39 19  1 14 14 19 43  1 19\n",
            "  43 53 23 19 46 32 39 23 29 19 24 44  1 19 11  9]\n",
            " [45  1 47 23 29 19 21 32 51  4  1  9 23 19 43 11 50 23 46 19 32 43 19 11\n",
            "  55 39  6  1 47 23 55 46 19 43 53 23 19 17  1 51]\n",
            " [26 26  8 35  0 31  1 46 43 51 18 19 14  9  1 16 19 43 53 23 19 32 16 47\n",
            "   9 32 55 43 46 19 44 53 23 55 19 43 53 23 18 47]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CQR06a6BnTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, chars, hidden_size, n_layers=2, drop_out=0.5, lr= 0.001):\n",
        "\n",
        "        super(CharLSTM, self).__init__()\n",
        "\n",
        "        # Set all the hyperparameters of your network\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_out = drop_out\n",
        "        self.lr = lr\n",
        "\n",
        "        # set vocabulary and get indices for these\n",
        "        self.chars = chars\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {w : i for i, w in self.int2char.items()}\n",
        "\n",
        "        # define the lstm network, this outputs the next char and cell state and hidden state\n",
        "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=hidden_size, num_layers=n_layers, dropout=drop_out,\n",
        "                       batch_first=True)\n",
        "        # add dropout\n",
        "        self.drop_out = nn.Dropout(drop_out)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, len(chars))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, x, h_0):\n",
        "        \"\"\"compute current and hidden units, stack outputs and pass it to linear layer\"\"\"\n",
        "\n",
        "        x, (h, c) = self.lstm(x, h_0)\n",
        "\n",
        "        x = self.drop_out(x)\n",
        "\n",
        "        # reshape x: stack the predicted and hidden state outputs\n",
        "        # cause fully-connected.\n",
        "        x = x.view(x.size()[0]*x.size()[1], self.hidden_size)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x, (h, c)\n",
        "\n",
        "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
        "        \"\"\"given a character, predict the next character in the sequence.\"\"\"\n",
        "        \n",
        "        if cuda:\n",
        "          self.cuda()\n",
        "        else:\n",
        "          self.cpu()\n",
        "            \n",
        "\n",
        "        # Initialize hidden state\n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "\n",
        "        # get the integer of character.\n",
        "        x = np.array([[self.char2int[char]]])\n",
        "\n",
        "        # one_hot_encode\n",
        "        x = one_hot_encode(x, len(self.chars))\n",
        "\n",
        "        # convet to tensor\n",
        "        one_hot = torch.from_numpy(x)\n",
        "        \n",
        "        if cuda:\n",
        "          one_hot = one_hot.cuda()\n",
        "\n",
        "        # create a tuple of the hidden state\n",
        "        # this is what LSTM expects\n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = self.forward(one_hot, h)\n",
        "\n",
        "        # Prob distribution over all the characters\n",
        "        probs = F.softmax(out, dim=1).data\n",
        "\n",
        "        # convert back prob to cpu if model was\n",
        "        # set to gpu\n",
        "        \n",
        "        if cuda:\n",
        "          probs = probs.cpu()\n",
        "\n",
        "        # if top number of preds to get\n",
        "        # wasn't pass, take the distribution over whole character length\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(self.chars))\n",
        "        else:\n",
        "            probs, top_ch = probs.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "        # reduce dims of size 1\n",
        "        probs = probs.numpy().squeeze()\n",
        "\n",
        "        char = np.random.choice(top_ch, p=probs/probs.sum())\n",
        "\n",
        "        return self.int2char[char], h\n",
        "\n",
        "    def init_weights(self):\n",
        "\n",
        "        initrange = 0.1\n",
        "\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "\n",
        "    def init_hidden(self, n_seqs):\n",
        "\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.hidden_size).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.hidden_size).zero_())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apZirO61O9Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data, epochs=10, n_seqs=10, n_steps=40, lr=0.001, clip=5, val_frac= 0.1, cuda=False,\n",
        "          print_every=10):\n",
        "\n",
        "    \"\"\" model: the model to b trained\n",
        "        data: the data on which we train\n",
        "        epochs: number of epochs to train for\n",
        "        n_seqs\" number of sequences in our batch\n",
        "        n_steps: time step for each sequence\n",
        "        lr: learning rate\n",
        "        clip: value used to clip the network gradient to prevent exploding gradeint.\n",
        "        val_frac: the fraction of data used for validation\n",
        "        print_every: the number of seconds for which we print out model statistics\n",
        "    \"\"\"\n",
        "\n",
        "    # change model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # define optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "\n",
        "    # trin and validation split\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if cuda:\n",
        "      model.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(model.chars)\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # initialize hidden layer of the model\n",
        "        h = model.init_hidden(n_seqs)\n",
        "\n",
        "        # loop over batches\n",
        "        for x, y in get_batches(data, n_seqs, n_steps):\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "            # one hot encode\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "\n",
        "            # convert to tensors\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            # move inputs and targets to cuda\n",
        "            \n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                \n",
        "            # New hidden state being created to prevented backpropogating through the \n",
        "            # entire history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero out gradient to prevent accumulation\n",
        "            model.zero_grad()\n",
        "\n",
        "            # get output and hidden\n",
        "            out, h = model.forward(inputs, h)\n",
        "            loss = criterion(out, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
        "\n",
        "            # backpropogate loss\n",
        "            loss.backward()\n",
        "\n",
        "            # use gradient clipping to prevent exploding gradient\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # take a step in the los surface\n",
        "            optimizer.step()\n",
        "\n",
        "            if counter % print_every ==0:\n",
        "\n",
        "                # initilize hidden state for validation\n",
        "                val_hidden = model.init_hidden(n_seqs)\n",
        "                val_losses = []\n",
        "\n",
        "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
        "\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    val_hidden = tuple([each.data for each in val_hidden])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    \n",
        "                    if cuda:\n",
        "                      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                      \n",
        "                    out, val_hidden = model.forward(inputs, val_hidden)\n",
        "\n",
        "                    val_loss = criterion(out, targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor))\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
        "                     \"Step: {}...\".format(counter),\n",
        "                     \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                     \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cocjEMcJCoQ-",
        "colab_type": "code",
        "outputId": "34669673-5c55-47d1-8953-98e6f0701461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model = CharLSTM(chars, hidden_size=512, n_layers=2)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharLSTM(\n",
              "  (lstm): LSTM(58, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (drop_out): Dropout(p=0.5)\n",
              "  (fc): Linear(in_features=512, out_features=58, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttvcu9DVKO03",
        "colab_type": "code",
        "outputId": "0a59a21e-cba7-4437-fde6-69abc7dd984a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "n_seqs, n_steps = 128, 100\n",
        "train(model, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/25... Step: 10... Loss: 3.4311... Val Loss: 3.4529\n",
            "Epoch: 2/25... Step: 20... Loss: 3.2848... Val Loss: 3.3369\n",
            "Epoch: 3/25... Step: 30... Loss: 3.2019... Val Loss: 3.2092\n",
            "Epoch: 4/25... Step: 40... Loss: 3.0406... Val Loss: 3.0688\n",
            "Epoch: 5/25... Step: 50... Loss: 2.9227... Val Loss: 2.9252\n",
            "Epoch: 6/25... Step: 60... Loss: 2.7988... Val Loss: 2.8039\n",
            "Epoch: 7/25... Step: 70... Loss: 2.7149... Val Loss: 2.7215\n",
            "Epoch: 8/25... Step: 80... Loss: 2.6249... Val Loss: 2.6663\n",
            "Epoch: 9/25... Step: 90... Loss: 2.5707... Val Loss: 2.6180\n",
            "Epoch: 10/25... Step: 100... Loss: 2.5670... Val Loss: 2.5751\n",
            "Epoch: 10/25... Step: 110... Loss: 2.4933... Val Loss: 2.5237\n",
            "Epoch: 11/25... Step: 120... Loss: 2.3985... Val Loss: 2.4964\n",
            "Epoch: 12/25... Step: 130... Loss: 2.3454... Val Loss: 2.4608\n",
            "Epoch: 13/25... Step: 140... Loss: 2.3752... Val Loss: 2.4306\n",
            "Epoch: 14/25... Step: 150... Loss: 2.3003... Val Loss: 2.3989\n",
            "Epoch: 15/25... Step: 160... Loss: 2.3240... Val Loss: 2.3680\n",
            "Epoch: 16/25... Step: 170... Loss: 2.3046... Val Loss: 2.3443\n",
            "Epoch: 17/25... Step: 180... Loss: 2.2628... Val Loss: 2.3193\n",
            "Epoch: 18/25... Step: 190... Loss: 2.2062... Val Loss: 2.3045\n",
            "Epoch: 19/25... Step: 200... Loss: 2.1814... Val Loss: 2.2759\n",
            "Epoch: 20/25... Step: 210... Loss: 2.2370... Val Loss: 2.2566\n",
            "Epoch: 20/25... Step: 220... Loss: 2.1925... Val Loss: 2.2401\n",
            "Epoch: 21/25... Step: 230... Loss: 2.0647... Val Loss: 2.2207\n",
            "Epoch: 22/25... Step: 240... Loss: 2.0481... Val Loss: 2.2185\n",
            "Epoch: 23/25... Step: 250... Loss: 2.0902... Val Loss: 2.1993\n",
            "Epoch: 24/25... Step: 260... Loss: 2.0257... Val Loss: 2.1894\n",
            "Epoch: 25/25... Step: 270... Loss: 2.0597... Val Loss: 2.1630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hn87xwwC-nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"lstm.net\"\n",
        "\n",
        "checkpoint = {'n_hidden': model.hidden_size, \n",
        "             'n_layers': model.n_layers, \n",
        "             'droput': model.drop_out, \n",
        "             'state_dict': model.state_dict(), \n",
        "             'toekns': model.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqJhQ6ZTSFMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model, size, prime='The', top_k=None, cuda=False):\n",
        "        \n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    # Prime characeters are starting points of some data one can use\n",
        "    chars = [ch for ch in prime]\n",
        "    \n",
        "    h = model.init_hidden(1)\n",
        "    \n",
        "    for ch in prime:\n",
        "        char, h = model.predict(ch, h, cuda=cuda, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        \n",
        "        char, h = model.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y9Wn7hvSGC5",
        "colab_type": "code",
        "outputId": "047ece68-8cef-47e2-8bca-c1de2e7d9a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "print(sample(model, 2000, prime='Horror', top_k=5, cuda=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Horrors  hom to beat the thryer then whers in the starding ane\n",
            "the runglys,, becting in hempant over houghing the range sood at hat sting thing of hought.\n",
            " thous a fat the some the backer is itsind in\n",
            "thing he mean. It thar though the bort.     continuing thing\n",
            "to his oun they.CHEFThes     continuing\n",
            " that in the bage.CHEF     continuing\n",
            "his this and aroust here and the strenting.         hermore.CHee ant as the bott outhathes a frachandeds        sheme and then toware a boods to hin arears that ane a the sonder somen on a tring the string.   M.D. VIEW ON WILLARDWellard.The Coptain shim some thing the rear.\n",
            "They were  starteds the this his of his are\n",
            "tring.CHOINEWhit's a the bart tries. Theyerants in out a frothong. HE  ON SHE  THe thing sellien and ano and at and cante the brich.    M.V.COAN T    continuing out the strening inther thas as to brow thengede the sand burt stickes to ser a to thit.CHIEF hat's to be the ridg take atowing.       cherrige ant the beer the stand tancer. It's a barting to sheromer are the the wold willong it the rond the bouts onteres\n",
            "theming in the sariges and the brat if town the seep of hems ofthing to the sold wer intat of ithick.CALENAN.     hallsing the ristel ands and his ithor that\n",
            "and the mover in the back the boods ate hame\n",
            "and the storter there all\n",
            "to the back.          here\n",
            "tange the sheres of the reant the rovaly to sore in to the the wer a butt in tho tall the throughe.WILLARD V.O.      thing sto shour he sell we ling the to towant.      the men ters\n",
            "a the ras on the solle this and allowhed. Willard theys ano allyowe oun athe sean oun a shough the stack thenghand thend the boand thene what all the simp ont ort a methe to the tone. We contanting to the to bethe..  then in the mant are thow and comentirs      continu in thrunger. I was the wanked a bandinting and the wourd and sores of the riss and to and shit.  MOD. SHOTh the recone ous the than back and ather a man thing.  ho distares and towhing to arous        hire\n",
            "semering ast to \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgBZ0l75Q8bD",
        "colab_type": "code",
        "outputId": "8a800920-fa24-4e58-b11d-4e7f4c4a99c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "print(sample(model, 5000, prime=\"doors\", top_k=5, cuda=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doors. Thing he man shat an       crepting in hissen and the sileter the riget the mant. The sore the seep. They aromald bucking the sand to santont      chon the\n",
            "rands of here.WILLARDI the strom in.KILGOREThat his sold of the bound thene with the rivile..     conthinging.The Canto andstall stats.Willard, stars ant a micet in it andit  a ment to the sontowhere this are to and beang. I cond of to met and the somento and saling.WILLARDWhat the rear. The raverered, boting the te the tance thingen thing he conds our the singlo anothes ard the bund off ham ant to the racking..Willard to sties the right hore shoth the sear into his songing and a back of ancathe houd on the tore. I'd gottar int of here the sald thing the ristrom his a dank aloont. The Callare.Willard looks at theme armen arouther oun that the conce. We the munted with the clacher stomess thime and and bot this the wat in han bereans an the crach the stare sing the thater the boat.  the mont inte this bouts and sond of the romes ove are\n",
            "shard ard and sillongher the batten of the bort on of he wart to sammet thinge the monting and the wat the want the beat he sood.WILLARD     that hus fent the tone to they wat he canters and shistint the sore aromer to the woll the mone sim.WILLARDI       con in.  the tores the som the ridger andes in itho stind the rushe shass a mive of him is in tho here\n",
            "a sat a mean intoming to some the some to the river the wind ano the tange ano the rean.\n",
            "To men is a the\n",
            "war seat offiler the tare an the ruck of the tore is a the care and thes with of hat. Wellard is a buck and ales and wit and strangs.KILGORE     continuing. The Chef in thatee. Willard the stick and stops offrimed. Willard's boant she panses one ther. There a coren it takes, the sam. I mas  a to and the bard sted on the rand butthit it thing the\n",
            "will thour ithen.CHEFI stores the tare and seards  whind his thing to she stare sit on the\n",
            "comande an thand the sittere on and the sord the seans and whate sittirn  whethe his and wears on the bot out\n",
            "on thourding on  tren a here the sores to bangertile on itho ghe\n",
            "thound of\n",
            "the winer of hems it his sing thiment and becanden at in the tolly to shanks. The Cherean sates in the\n",
            "med. I wat he wend a tere cammer in the beered. They\n",
            "rever thry grand the seen.  the brother\n",
            "sared a mor through the roving. Willard stals the munt of him the sally of the wancone him the wally fore to thing ard ardondintires\n",
            " ano to bitt.WILLARDWelland over the stom ind the bott.WILLARD     has and tels\n",
            "one his on the somalles the beack.CHEFWho he hiss on the\n",
            "steds to backs  the\n",
            "wording and thiss of the tolotel the sond the wething. Willard then arough ortheres ally that ofer the rids an the boots of the sind though the routs the beant. He sunted and stres thing\n",
            "his seall wooking hink han souren thes whother the man on a camstan       couning as the meace\n",
            "tarne the tonghine him the buce.        the\n",
            "blece the tare and the mives has of the roads this has somp and the rigut to has and thing stanking ano the back sin the backsted to shather.       he\n",
            "when is and theres\n",
            "ato his the sar seen ins on the saret and there sing on thase.CHEFWhith how and a there a sard fillard ofer to this\n",
            "tole of her wolde some the the bear.       theser the sing ho stinting and this ander.WILLARD     continuing.CHEF     stremiting ane a tange of thim.Chiffich.  then the stands to bo sheme the shim. The banding an a his thas the thin the sands ald as it his is one and.HILLARDWhat a to boonded wat the mittring oun ane hurd there shemes, the\n",
            "sore and wand the sinto to the somer.  thougher here there wase and and sours the\n",
            "wires. Willard thoughs  the race the the to hem. He move and\n",
            "are the site to to a droothing his. They's cante in of they at thas in inthongers andinger the bood to seed. The mon are the stard a dich and cringed on the batine ore and\n",
            "starining of a trees and here wish siligher. I con'te\n",
            "bood.      the\n",
            "seres and the stones at in thingen of the to the rear the sealle whise that and thie they contire.. I was ane she town and a cars a beand ant his issed anound the recont around the burke thing an a shist is he wand that is\n",
            "thand the sim arding tite an the reats, there then trankes anound and sout of threache he canter ther the sing in the the were out.WILLARD V.O.TO SHE The bealds at ou histan to bur stering the wirh       hellarging aneron the barks a doont. They tor ard the redire.\n",
            "We can on ond hemeriche seer the rachen.WILLARD V.O.     cantinuing and at as out his. We ane sares\n",
            "overitith is a the carmares arount.     contening thimentant the reatire out and sim.We\n",
            "the means. They as and be carner arear towir on the beat.We cans at and solm there and are the bectarns and bur seald to thes are and contorith  wonge arone the bate..     he purns on is hom he mos         he wis ond this.  MED.  HE TORESEANThere san seatere. He sour the boand to the wer out ald\n",
            "boting.     canting and this\n",
            "tiles of has sine this ald ster to to the wis one this\n",
            "and brint  hat somer tints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk4q7M1_XCzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}